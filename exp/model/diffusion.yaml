# @package _global_

model:
  _target_: main.module_diffusion.Model
  lr: 1e-4
  lr_beta1: 0.95
  lr_beta2: 0.999
  lr_eps: 1e-6
  lr_weight_decay: 1e-3

  model:
    _target_: audio_diffusion_pytorch.DiffusionModel
    net_t:
      _target_: audio_diffusion_pytorch.UNetV0
      _partial_: True
    in_channels: 1
    channels: [8, 32, 64, 128, 256, 512, 1024, 1024]
    factors:    [ 1, 4, 4, 4, 2, 2, 2, 2]
    items:      [ 1, 2, 2, 2, 2, 2, 2, 4]
    attentions: [ 0, 0, 0, 0, 1, 1, 1, 1]
    attention_heads: 8
    attention_features: 64
    context_channels: [2, 8, 16, 32, 64, 128, 256, 256]
    diffusion_t:
      _target_: audio_diffusion_pytorch.VDiffusion
      _partial_: True
    sampler_t:
      _target_: audio_diffusion_pytorch.VSampler
      _partial_: True
    use_embedding_cfg: True
    embedding_max_length: 1
    embedding_features: 512
    cross_attentions: [ 1, 1, 1, 1, 1, 1, 1, 1 ]

  onsets_encoder:
    _target_: audio_encoders_pytorch.Encoder1d
    in_channels: 1
    channels: 2
    multipliers: [1, 1, 4, 8, 16, 32, 64, 128, 128]
    factors: [1, 4, 4, 4, 2, 2, 2, 2]
    num_blocks: [2, 2, 2, 2, 2, 2, 2, 2]
    resnet_groups: 2
    patch_size: 1

  embedder:
    _target_: laion_clap.CLAP_Module
    enable_fusion: False
    amodel: 'HTSAT-tiny'
  embedder_checkpoint: null